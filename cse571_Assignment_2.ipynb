{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "b8ySVcf_RZgH"
      ],
      "mount_file_id": "1sCt0uwKM8aNlW3dYPLwRKhLbOuol7z0o",
      "authorship_tag": "ABX9TyOxEs9w0pemRSUkZ+n3OUDE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nk106/Collision-Prediction-RL/blob/main/cse571_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz5czljw3VwU"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SzXMAJ04V_e",
        "outputId": "7b8ad997-065b-4892-e15b-c51f4d752297"
      },
      "source": [
        "%cd drive/MyDrive/School/CSE 571/Assignment2/assignment2_part1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/School/CSE 571/Assignment2/assignment2_part1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YakN58lV-ksW",
        "outputId": "f4fd6718-5ee2-4a52-b0d2-a7feb5e80304"
      },
      "source": [
        "#Python libraries: cython matplotlib sklearn scipy pymunk pygame pillow numpy noise torch\n",
        "!pip install pymunk==5.7.0\n",
        "!pip install noise\n",
        "!pip install pillow\n",
        "!pip install pygame"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymunk==5.7.0\n",
            "  Downloading pymunk-5.7.0-py2.py3-none-manylinux1_x86_64.whl (538 kB)\n",
            "\u001b[?25l\r\u001b[K     |▋                               | 10 kB 26.6 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20 kB 30.7 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30 kB 19.0 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 51 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 61 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 81 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 92 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 286 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 296 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 307 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 317 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 327 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 337 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 348 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 358 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 368 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 378 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 389 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 399 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 409 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 419 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 430 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 440 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 450 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 460 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 471 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 481 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 491 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 501 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 512 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 522 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 532 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 538 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi!=1.13.1 in /usr/local/lib/python3.7/dist-packages (from pymunk==5.7.0) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi!=1.13.1->pymunk==5.7.0) (2.21)\n",
            "Installing collected packages: pymunk\n",
            "Successfully installed pymunk-5.7.0\n",
            "Collecting noise\n",
            "  Downloading noise-1.2.2.zip (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 5.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: noise\n",
            "  Building wheel for noise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for noise: filename=noise-1.2.2-cp37-cp37m-linux_x86_64.whl size=64368 sha256=96458030d96a5536d0a5b6babb8932c8a15d22cf84c4d2e36d118c66470a4fa7\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/4f/1d/3e94460751c993553ba1a52b8e571ba6510701b4d0a68ffeea\n",
            "Successfully built noise\n",
            "Installing collected packages: noise\n",
            "Successfully installed noise-1.2.2\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Collecting pygame\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 1.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owiCgslKErXl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1af69fd5-6422-43e0-df4f-a0903433c12f"
      },
      "source": [
        "!pip uninstall scikit-learn -y\n",
        "!pip install scikit-learn==0.21.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: scikit-learn 0.22.2.post1\n",
            "Uninstalling scikit-learn-0.22.2.post1:\n",
            "  Successfully uninstalled scikit-learn-0.22.2.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_2dz-xrQ62B"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ['SDL_VIDEODRIVER'] = 'dummy'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yln-_qtnUHYm",
        "cellView": "form"
      },
      "source": [
        "#@title testing numpy array concnat stuff \n",
        "import numpy as np\n",
        "tAy = [1,2,3,4,5,6,7]\n",
        "print(tAy)\n",
        "bAy = 100\n",
        "cAy = [200]\n",
        "\n",
        "dAy = np.asarray(bAy)\n",
        "print (dAy)\n",
        "#cncat = np.concatenate((tAy,dAy,cAy))\n",
        "cncat = np.append(tAy,bAy)\n",
        "dncat = cncat \n",
        "\n",
        "cncat.append(tAy)\n",
        "print(\"cncat: \")\n",
        "print(cncat)\n",
        "print(\"cncat: \")\n",
        "\n",
        "#apxay = np.apply_along_axis(cncat,cncat)\n",
        "apxay = np.vstack((cncat,cncat))\n",
        "print(apxay)\n",
        "\n",
        "print('')\n",
        "stckA = np.vstack((apxay,cncat))\n",
        "print(stckA)\n",
        "\n",
        "print('')\n",
        "stckB = np.vstack((stckA,stckA))\n",
        "print(stckB)\n",
        "\n",
        "stckB[-1][-1] = 9\n",
        "print(stckB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY3Je545ZusT"
      },
      "source": [
        "empy = [0]*9\n",
        "empy = (np.vstack((empy,stckB)))\n",
        "print(empy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUBkf8rUi3rQ"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "#Submit only 100 samps BUT need to TRAIN on FAR MORE!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U-g4aIp5G0q",
        "cellView": "form"
      },
      "source": [
        "#@title **MAY NEED TO ACTUALLY USE TO SUBMIT??** Data Collection --> CSV .py\n",
        "# ************** STUDENTS EDIT THIS FILE **************\n",
        "### ****************************************************************Collect_Data.py*********\n",
        "from SteeringBehaviors import Wander\n",
        "import SimulationEnvironment as sim\n",
        "import numpy as np\n",
        "\n",
        "def collect_training_data(total_actions):\n",
        "    #set-up environment\n",
        "    sim_env = sim.SimulationEnvironment()\n",
        "\n",
        "    #robot control\n",
        "    action_repeat = 100\n",
        "    steering_behavior = Wander(action_repeat)\n",
        "\n",
        "    num_params = 7\n",
        "    #STUDENTS: network_params will be used to store your training data\n",
        "    # a single sample will be comprised of: sensor_readings, action, collision\n",
        "    network_params = [0]*7\n",
        "\n",
        "\n",
        "    for action_i in range(total_actions):\n",
        "        progress = 100*float(action_i)/total_actions\n",
        "        print(f'Collecting Training Data {progress}%   ', end=\"\\r\", flush=True)\n",
        "\n",
        "        #steering_force is used for robot control only\n",
        "        action, steering_force = steering_behavior.get_action(action_i, sim_env.robot.body.angle)\n",
        "\n",
        "        for action_timestep in range(action_repeat):\n",
        "            if action_timestep == 0:\n",
        "                _, collision, sensor_readings = sim_env.step(steering_force)\n",
        "                # print(action_i)       #test\n",
        "                # print(sensor_readings)#test\n",
        "            else:\n",
        "                _, collision, _ = sim_env.step(steering_force)\n",
        "\n",
        "            if collision:\n",
        "                steering_behavior.reset_action()\n",
        "                #STUDENTS NOTE: this statement only EDITS collision of PREVIOUS action\n",
        "                #if current action is very new.\n",
        "                if action_timestep < action_repeat * .3: #in case prior action caused collision\n",
        "                    print(\"******* collision Prior ******* \")\n",
        "                    network_params[-1][-1] = collision\n",
        "\n",
        "          #UNCOMMENT-->          network_params[-1][-1] = collision #share collision result with prior action\n",
        "                    # print(\"Collision: \"+str(collision))\n",
        "                    # print(\"action_timestep\"+str(action_timestep))\n",
        "                    # print(\"action: \"+str(action))\n",
        "                break\n",
        "\n",
        "\n",
        "        #STUDENTS: Update network_params.\n",
        "        sensor_readings = np.append(sensor_readings,action)\n",
        "        sensor_readings = np.append(sensor_readings,collision)\n",
        "        network_params = np.vstack((network_params,sensor_readings))\n",
        "\n",
        "\n",
        "        # print(\"Action i: \"+str(action_i))       #test\n",
        "        # print(\"Sensor Readings: \"+str(sensor_readings))#test\n",
        "        # print(\"action: \"+str(action))\n",
        "        # print(\"Collision: \"+str(collision))\n",
        "\n",
        "\n",
        "    #ANSWER HERE:   network_params = [sensor_readings][action][collision]\n",
        "\n",
        "\n",
        "    #STUDENTS: Save .csv here. Remember rows are individual samples, the first 5\n",
        "    #columns are sensor values, the 6th is the action, and the 7th is collision.\n",
        "    #Do not title the columns. Your .csv should look like the provided sample.\n",
        "\n",
        "    print(network_params)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    total_actions = 100\n",
        "    collect_training_data(total_actions)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPZ6pB033jtC"
      },
      "source": [
        "with open(\"saved/2500_samps.csv\",\"r\") as source:\n",
        "    lines = [line for line in source]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4r69oCI3tMs"
      },
      "source": [
        "import random\n",
        "random_choice = random.sample(lines, 100)\n",
        "\n",
        "with open(\"100_samps.csv\", \"w\") as sink:\n",
        "    sink.write(\"\".join(random_choice))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVyvYKYrh1cT"
      },
      "source": [
        "#CSV given has 11000 Rows(!), 7 columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZasclL-gzJl"
      },
      "source": [
        "print(network_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2b9zPDkFCaH",
        "outputId": "673b1237-8d4f-4b3b-d2fc-46584dc8bcd7"
      },
      "source": [
        "!pip install scikit-learn==0.21.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.21.1\n",
            "  Downloading scikit_learn-0.21.1-cp37-cp37m-manylinux1_x86_64.whl (6.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.1) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.1) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.1) (1.19.5)\n",
            "Installing collected packages: scikit-learn\n",
            "Successfully installed scikit-learn-0.21.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADATjy8humCx",
        "cellView": "form"
      },
      "source": [
        "#@title train-test-split DataLoader.py\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch.utils.data.dataset as dataset\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "from sklearn.model_selection import train_test_split #added.. for now\n",
        "\n",
        "\n",
        "class Nav_Dataset(dataset.Dataset):\n",
        "    def __init__(self):\n",
        "       #UNCOMMENT self.data = np.genfromtxt('saved/training_data.csv', delimiter=',')\n",
        "      #OLDnk \n",
        "      self.data = np.genfromtxt('saved/testsheet.csv', delimiter=',')\n",
        "      \n",
        "# STUDENTS: it may be helpful for the final part to balance the distribution of your collected data\n",
        "\n",
        "        # normalize data and save scaler for inference\n",
        "       self.scaler = MinMaxScaler()\n",
        "       self.normalized_data = self.scaler.fit_transform(self.data) #fits and transforms\n",
        "       pickle.dump(self.scaler, open(\"saved/scaler.pkl\", \"wb\")) #save to normalize at inference\n",
        "\n",
        "    def __len__(self):\n",
        "# STUDENTS: __len__() returns the length of the dataset\n",
        "        return(len(self.data))\n",
        "        #pass\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if not isinstance(idx, int):\n",
        "            idx = idx.item()\n",
        "# STUDENTS: for this example, __getitem__() must return a dict with entries {'input': x, 'label': y}\n",
        "# x and y should both be of type float32. There are many other ways to do this, but to work with autograding\n",
        "# please do not deviate from these specifications.\n",
        "\n",
        "      #i think i need to Define a Dictionary? where X[6] = Input and Y[1] = Output, and then Return that Dict? \n",
        "      #but what does 'idx' do? WHat's the difference between \"item()\" && \"items()\"!? \n",
        "      #idx.item is a  \n",
        "        x = idx   #should this be 'normalized_data' or just data?? \n",
        "        #y = self.normalized_data[idx]# OR self.normalized_data.idx???\n",
        "        y = self.data[idx]\n",
        "        returndict = {'input':idx,'label':y}    #for some reason i had to reverse x & y here....\n",
        "        return(returndict)\n",
        "        #return(self.normalized_data.idx)\n",
        "  # the get item function should work on the normalized data as later on that its what would be fed as input \n",
        "  #WHAT ARE THE INPUTS!?!? IT IS THE ARRAY ----- forget this: is it the whole array of 6 values or just 1 index value? instructions say x should be type float32.. can it be an array type float32? \n",
        "\n",
        "\n",
        "class Data_Loaders():\n",
        "    def __init__(self, batch_size):\n",
        "        self.nav_dataset = Nav_Dataset()\n",
        "# STUDENTS: randomly split dataset into two data.DataLoaders, self.train_loader and self.test_loader\n",
        "# make sure your split can handle an arbitrary number of samples in the dataset as this may vary\n",
        "        # test_size = int((len(self.nav_dataset))*0.2)\n",
        "        # train_size = len(self.nav_dataset) - test_size  \n",
        "\n",
        "        #from Large Matrix to Smaller One: smaller = larger[startRow:startRow+size,startCol:startCol+size]\n",
        "                      #i think this is the Matrix i should be using? \n",
        "        inputs = self.nav_dataset.data[0:len(self.nav_dataset),0:6]  #i think this is right?? not sure if Cols should be 0:5 or 1:6\n",
        "                                                #am i using .len correctly?  \n",
        "        outputs = self.nav_dataset.data[0:len(self.nav_dataset),-1] #should be Last column of Dataset\n",
        "\n",
        "       # print(self.nav_dataset.normalized_data)\n",
        "       # print(inputs)\n",
        "                #MAY NEED TO UPDATE TRAIN_TEST_SPLIT as i need to import a package not included in given .py file\n",
        "        input_train,input_test,output_train,output_test = train_test_split(inputs,outputs,stratify = outputs) #need to Update Stratify to dummy array of split 0's & 1's\n",
        "              #i could also just have a 'train' , 'test' _,_ right?? to get 2 instead of 4 Tensors.. \n",
        "\n",
        "          #DataLoader seems to just take one array as input, but i have 'input train' & 'output train' as different.. should i combine them? \n",
        "          #or can DataLoader take both x,y at the same time? \n",
        "        # train = data_utils.TensorDataset(features, targets)     #might have to use different Class here.. \n",
        "        # train_loader = data_utils.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "        input_train = torch.from_numpy(input_train)\n",
        "        output_train = torch.from_numpy(output_train)\n",
        "        train = torch.utils.data.TensorDataset(input_train, output_train)     #might have to use different Class here.. \n",
        "\n",
        "        trainers,testers = train_test_split(self.nav_dataset) #This works BUT now idk how to get Stratify to WOrk!\n",
        "                                                                  #need to make sure Stratify is Working && that it takes in correct Mix Array\n",
        "\n",
        "\n",
        "        print(trainers)\n",
        "\n",
        "        #self.train_loader = data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "        self.train_loader = data.DataLoader(trainers, batch_size=batch_size, shuffle=True)\n",
        "            #THIS works, meaning that the DataLoader has to WrapAround the DATASET class.. somehow!\n",
        "        self.test_loader = data.DataLoader(testers, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        input_test = torch.from_numpy(input_test)\n",
        "        output_test = torch.from_numpy(output_test)\n",
        "        test = data.TensorDataset(input_test, output_test)\n",
        "       # self.test_loader = data.DataLoader(test, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # self.train_loader = data.DataLoader(,batch_size#FIX)\n",
        "        # self.test_loader = data.DataLoader(,batch_size#FIX)\n",
        "\n",
        "        #NOTE: \"Finally had to switch to train_test_split to get the autograder working, but keeping this as a placeholder incase others had issues with this as well.\n",
        "        #1. *somehow* split Dataset into 6 columns = Inputs, Last 1 column = Output. OR use nav_dataset.GETITEM to get the Inputs & Outputs for the self.nav_dataset\n",
        "        #2. use train_test_split(Inputs,Outputs,stratify = outputs) OR could use \"torch.utils.data.random_split\"...\n",
        "        #      = input_train, input_test, output_train, output_test\n",
        "        #2a. 'stratify = array_to_match' ... so i could use an array w/ even split of 0's & 1's to try and approximate the Stratification \n",
        "        #3. *somehow(i think(yes!) using 'DataLoader'* set self.Train_loader as (input_train,output_train) && self.Test_loader as (input_test,output_test) \n",
        "\n",
        "\n",
        "def main():\n",
        "    batch_size = 16\n",
        "    data_loaders = Data_Loaders(batch_size)\n",
        "    # STUDENTS : note this is how the dataloaders will be iterated over, and cannot be deviated from\n",
        "    for idx, sample in enumerate(data_loaders.train_loader):\n",
        "        _, _ = sample['input'], sample['label']\n",
        "    for idx, sample in enumerate(data_loaders.test_loader):\n",
        "        _, _ = sample['input'], sample['label']\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zJL1xl_yvB3",
        "outputId": "0a79f239-39c2-47c1-91e5-11d27b170e86"
      },
      "source": [
        "class MyObject:\n",
        "     def __len__(self):\n",
        "          return 100\n",
        "\n",
        "a = MyObject()\n",
        "print(len(a))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8ySVcf_RZgH"
      },
      "source": [
        "# old Part 2:  - Data_Loaders.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aYLNmcvofQ8",
        "outputId": "915bc917-c33e-4581-99eb-1bd27f16be53"
      },
      "source": [
        "%cd drive/MyDrive/School/CSE 571/Assignment2/assignment2_part1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/School/CSE 571/Assignment2/assignment2_part1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqeKT0zLTlPN",
        "outputId": "97eaea79-bc8c-44e5-b819-bee177c8d6e8"
      },
      "source": [
        "%cd ../\n",
        "%cd assignment2_part1/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/School/CSE 571/Assignment2\n",
            "/content/drive/My Drive/School/CSE 571/Assignment2/assignment2_part1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohWnbwglLZHF",
        "outputId": "02dc778d-0852-4cc0-f7b4-1911dcd93fc2"
      },
      "source": [
        "!python --version #\"in the end I had to create another environment running python 3.7.11\"\n",
        "pymunk==5.7.0\n",
        "Python 3.7.12\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLhC2dtaI9Uz",
        "cellView": "form"
      },
      "source": [
        "#@title prev Data_Loader.py 'final'\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch.utils.data.dataset as dataset\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "#from sklearn.model_selection import train_test_split #added.. for now\n",
        "\n",
        "\n",
        "class Nav_Dataset(dataset.Dataset):\n",
        "    def __init__(self):\n",
        "       #*********UNCOMMENT \n",
        "       #self.data = np.genfromtxt('saved/training_data.csv', delimiter=',')\n",
        "      #OLDNK self.data = np.genfromtxt('saved/testsheet.csv', delimiter=',')\n",
        "      #2nd: self.data = np.genfromtxt('saved/2500samps.csv', delimiter=',')\n",
        "     #  self.data = np.genfromtxt('saved/TODO REDUCE SAMPLE training_data.csv', delimiter=',')\n",
        "       \n",
        "# STUDENTS: it may be helpful for the final part to balance the distribution of your collected data\n",
        "       self.data = np.float32(self.data) #TEST\n",
        "\n",
        "        # normalize data and save scaler for inference\n",
        "       self.scaler = MinMaxScaler()\n",
        "       self.normalized_data = self.scaler.fit_transform(self.data) #fits and transforms\n",
        "       #pickle.dump(self.scaler, open(\"saved/scaler.pkl\", \"wb\")) #save to normalize at inference\n",
        "       torch.save(self.scaler, 'saved/scaler.pkl',_use_new_zipfile_serialization=False)\n",
        "\n",
        "    def __len__(self):\n",
        "# STUDENTS: __len__() returns the length of the dataset\n",
        "        return(len(self.data))\n",
        "        #pass\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if not isinstance(idx, int):\n",
        "            idx = idx.item()\n",
        "# STUDENTS: for this example, __getitem__() must return a dict with entries {'input': x, 'label': y}\n",
        "# x and y should both be of type float32. There are many other ways to do this, but to work with autograding\n",
        "# please do not deviate from these specifications.\n",
        "\n",
        "        x = self.normalized_data[idx][0:6] #.NORMALIZED OR DATA?? \n",
        "        y = self.normalized_data[idx][-1]\n",
        "        returndict = {'input':x,'label':y}   \n",
        "        return(returndict)\n",
        "\n",
        "\n",
        "class Data_Loaders():\n",
        "    def __init__(self, batch_size):\n",
        "        self.nav_dataset = Nav_Dataset()\n",
        "# STUDENTS: randomly split dataset into two data.DataLoaders, self.train_loader and self.test_loader\n",
        "# make sure your split can handle an arbitrary number of samples in the dataset as this may vary\n",
        "\n",
        "        outputs = self.nav_dataset.data[0:len(self.nav_dataset),-1] #should be Last column of Dataset\n",
        " \n",
        "    #*****FINAL ISSUE******: is splitting Train & Test to Keep 0's & 1's 'balanced' or something.. \n",
        "          #if using 'RandomSplit' instead (which'll not require the extra import pkg)..\n",
        "          # then you need to maks sure original Dataset is Balanced first & then the Split will be Balaned too\n",
        "        #trainers,testers = train_test_split(self.nav_dataset,stratify=outputs) #This works BUT now idk how to get Stratify to WOrk!\n",
        "#also need to split 80/20 probably?                          #need to make sure Stratify is Working && that it takes in correct Mix Array\n",
        "#        print(trainers)\n",
        "\n",
        "        test_size = int((len(self.nav_dataset))*0.2)\n",
        "        train_size = len(self.nav_dataset) - test_size\n",
        "\n",
        "        trainers,testers = data.random_split(self.nav_dataset,[train_size,test_size])\n",
        "\n",
        "        self.train_loader = data.DataLoader(trainers, batch_size=batch_size, shuffle=True)\n",
        "            #THIS works, meaning that the DataLoader has to WrapAround the DATASET class.. somehow!\n",
        "        self.test_loader = data.DataLoader(testers, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "def main():\n",
        "    batch_size = 16\n",
        "    data_loaders = Data_Loaders(batch_size)\n",
        "    # STUDENTS : note this is how the dataloaders will be iterated over, and cannot be deviated from\n",
        "    for idx, sample in enumerate(data_loaders.train_loader):\n",
        "        _, _ = sample['input'], sample['label']\n",
        "    for idx, sample in enumerate(data_loaders.test_loader):\n",
        "        _, _ = sample['input'], sample['label']\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5LIaQb5FyMb"
      },
      "source": [
        "# FINAL FINAL DATALOADER>PY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mg-E8zyFmjf0",
        "outputId": "1a801394-b3f0-4fc2-e991-2938c9edf106"
      },
      "source": [
        "%cd drive/MyDrive/School/CSE 571/Assignment2/assignment2_part1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/School/CSE 571/Assignment2/assignment2_part1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvEAezz_mkPt"
      },
      "source": [
        "#HELPER CELL - if Need to get back to 'part1' folder\n",
        "%cd ../\n",
        "%cd assignment2_part1/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8FdjhE6YKdg"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch.utils.data.dataset as dataset\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "\n",
        "class Nav_Dataset(dataset.Dataset):\n",
        "    def __init__(self):\n",
        "        #COMMENTING OUT to get FULL TRAINING CSV self.data = np.genfromtxt('saved/training_data.csv', delimiter=',')\n",
        "        self.data = np.genfromtxt('saved/TODO REDUCE SAMPLE training_data.csv', delimiter=',')\n",
        "# STUDENTS: it may be helpful for the final part to balance the distribution of your collected data\n",
        "\n",
        "        # normalize data and save scaler for inference\n",
        "        self.scaler = MinMaxScaler()\n",
        "        self.normalized_data = self.scaler.fit_transform(self.data) #fits and transforms\n",
        "        pickle.dump(self.scaler, open(\"saved/scaler.pkl\", \"wb\")) #save to normalize at inference\n",
        "        #REPLACED for Part 4\n",
        "        #torch.save(self.scaler, 'saved/scaler.pkl',_use_new_zipfile_serialization=False)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "# STUDENTS: __len__() returns the length of the dataset\n",
        "        return(len(self.data))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if not isinstance(idx, int):\n",
        "            idx = idx.item()\n",
        "# STUDENTS: for this example, __getitem__() must return a dict with entries {'input': x, 'label': y}\n",
        "# x and y should both be of type float32. There are many other ways to do this, but to work with autograding\n",
        "# please do not deviate from these specifications.\n",
        "        x = self.normalized_data[idx][0:6] #.NORMALIZED OR DATA?? \n",
        "        y = self.normalized_data[idx][-1]\n",
        "        x = np.float32(x)\n",
        "        y = np.float32(y)\n",
        "        returndict = {'input':x,'label':y}\n",
        "        return(returndict)   \n",
        "\n",
        "class Data_Loaders():\n",
        "    def __init__(self, batch_size):\n",
        "        self.nav_dataset = Nav_Dataset()\n",
        "# STUDENTS: randomly split dataset into two data.DataLoaders, self.train_loader and self.test_loader\n",
        "# make sure your split can handle an arbitrary number of samples in the dataset as this may vary\n",
        "        test_size = int((len(self.nav_dataset))*0.2)\n",
        "        train_size = len(self.nav_dataset) - test_size\n",
        "\n",
        "        trainers,testers = data.random_split(self.nav_dataset,[train_size,test_size])\n",
        "\n",
        "        self.train_loader = data.DataLoader(trainers, batch_size=batch_size, shuffle=True)\n",
        "            #THIS works, meaning that the DataLoader has to WrapAround the DATASET class.. somehow!\n",
        "        self.test_loader = data.DataLoader(testers, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "def main():\n",
        "    batch_size = 16\n",
        "    data_loaders = Data_Loaders(batch_size)\n",
        "    # STUDENTS : note this is how the dataloaders will be iterated over, and cannot be deviated from\n",
        "    for idx, sample in enumerate(data_loaders.train_loader):\n",
        "        _, _ = sample['input'], sample['label']\n",
        "    for idx, sample in enumerate(data_loaders.test_loader):\n",
        "        _, _ = sample['input'], sample['label']\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FrNzzTD4A-P",
        "cellView": "form",
        "outputId": "5e184917-9178-4f2e-aa41-e47493a5fee5"
      },
      "source": [
        "#@title Should Output \"0.21.1\" for scikit-learn\n",
        "#print('The scikit-learn version is {}.'.format(scikit-learn.__version__))\n",
        "!pip freeze | grep scikit-learn\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scikit-learn==0.21.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks4Ioq_t1zsa"
      },
      "source": [
        "# Part 3: Networks.py  \n",
        "##HERE is where you Adjust Model Arch, Submit final as .py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPO_DoBkR_Ht"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Action_Conditioned_FF(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "# STUDENTS: __init__() must initiatize nn.Module and define your network's\n",
        "# custom architecture\n",
        "        input_dim = 6\n",
        "        output_dim = 1 #i think? \n",
        "        # Linear function\n",
        "        self.layer1 = nn.Linear(input_dim, 40)  #WAS using 20 hidden dim here, maybe use 100??\n",
        "\n",
        "        # # Non-linearity                   NOT using Non-Linearity FOR NOW??\n",
        "        # self.sigmoid = nn.Sigmoid()   \n",
        "\n",
        "       # self.layer2 = nn.Linear(20, output_dim)\n",
        "        \n",
        "        # Linear function (readout)\n",
        "        self.layer2 = nn.Linear(40,40)  #WAS 20,20 && was not included before\n",
        "        self.layer3 = nn.Linear(40, output_dim)   #was 'layer2' before.. \n",
        "\n",
        "     #   pass\n",
        "\n",
        "    def forward(self, input):\n",
        "# STUDENTS: forward() must complete a single forward pass through your network\n",
        "# and return the output which should be a tensor\n",
        "        # y_predicted=self.layer1(input)\n",
        "        # output=torch.sigmoid(self.layer2(y_predicted))\n",
        "\n",
        "        out=self.layer1(input)\n",
        "        out = torch.relu(out)  #RELU MADE THE DIFFERENCE!!!!\n",
        "        out=self.layer2(out)\n",
        "        output = torch.sigmoid(self.layer3(out))\n",
        "\n",
        "       # print(type(output))\n",
        "        return output\n",
        "\n",
        "\n",
        "    def evaluate(self, model, test_loader, loss_function):\n",
        "# STUDENTS: evaluate() must return the loss (a value, not a tensor) over your testing dataset. Keep in\n",
        "# mind that we do not need to keep track of any gradients while evaluating the\n",
        "# model. loss_function will be a PyTorch loss function which takes as argument the model's\n",
        "# output and the desired output.\n",
        "        for indx,batchdict in enumerate(test_loader):\n",
        "          \n",
        "              in_batch = batchdict['input']\n",
        "                 #need to get Tensor from 'Inputs' --> model(inputs)\n",
        "              pred = torch.flatten(model(in_batch))\n",
        "   \n",
        "              label_batch = batchdict['label']\n",
        "             \n",
        "              loss = loss_function(pred, label_batch)  #need to use Tensor of 'Labels' here\n",
        "\n",
        "              loss_val = loss.item()\n",
        "\n",
        "        return loss_val #\n",
        "\n",
        "def main():\n",
        "    model = Action_Conditioned_FF()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPA0lZwyv4FA"
      },
      "source": [
        "#@title *'Networks.py' TO SUBMIT (w/o edits & comments)*\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Action_Conditioned_FF(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "# STUDENTS: __init__() must initiatize nn.Module and define your network's\n",
        "# custom architecture\n",
        "        input_dim = 6\n",
        "        output_dim = 1 \n",
        "        # Linear function\n",
        "        self.layer1 = nn.Linear(input_dim, 40) \n",
        "        # Linear function \n",
        "        self.layer2 = nn.Linear(40,40)  \n",
        "\n",
        "        self.layer3 = nn.Linear(40, output_dim)    \n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "# STUDENTS: forward() must complete a single forward pass through your network\n",
        "# and return the output which should be a tensor\n",
        "\n",
        "        out = self.layer1(input)\n",
        "        out = torch.relu(out)\n",
        "        out = self.layer2(out)\n",
        "        output = torch.sigmoid(self.layer3(out))\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "    def evaluate(self, model, test_loader, loss_function):\n",
        "# STUDENTS: evaluate() must return the loss (a value, not a tensor) over your testing dataset. Keep in\n",
        "# mind that we do not need to keep track of any gradients while evaluating the\n",
        "# model. loss_function will be a PyTorch loss function which takes as argument the model's\n",
        "# output and the desired output.\n",
        "        for indx,batchdict in enumerate(test_loader):\n",
        "          \n",
        "              in_batch = batchdict['input']\n",
        "                 #need to get Tensor from 'Inputs' --> model(inputs)\n",
        "              pred = torch.flatten(model(in_batch))\n",
        "   \n",
        "              label_batch = batchdict['label']\n",
        "             \n",
        "              loss = loss_function(pred, label_batch)  #need to use Tensor of 'Labels' here\n",
        "\n",
        "              loss_val = loss.item()\n",
        "\n",
        "        return loss_val \n",
        "\n",
        "def main():\n",
        "    model = Action_Conditioned_FF()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrnxeg7Jou6R",
        "outputId": "920c32e7-d74a-4fc3-b4ae-df91ae447b72"
      },
      "source": [
        "#@title Helper Cell: Test simple Input of Model\n",
        "x = torch.randn((10,6))\n",
        "model = Action_Conditioned_FF()\n",
        "model(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6099],\n",
              "        [0.5526],\n",
              "        [0.5126],\n",
              "        [0.6533],\n",
              "        [0.5554],\n",
              "        [0.5327],\n",
              "        [0.5599],\n",
              "        [0.5869],\n",
              "        [0.5396],\n",
              "        [0.5750]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfT_pmdCC3vf"
      },
      "source": [
        "# Part 4: train_model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5emyH3T2xy_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "outputId": "3a7ca548-3a8f-4f3b-88fc-39867f44f048"
      },
      "source": [
        "#UNCOMMENT when submitteing: from Data_Loaders import Data_Loaders\n",
        "#from Networks import Action_Conditioned_FF\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def train_model(no_epochs):\n",
        "\n",
        "    batch_size = 5  #<5 seems to be much better for Acc than >5 or something like 16 #taken from part 2 .py file\n",
        "      #WHAT EXACTLY DOES BATCH_SIZE DO & HOW DOES IT AFFECT PERFORMANCE?? \n",
        "    data_loaders = Data_Loaders(batch_size)\n",
        "    model = Action_Conditioned_FF()\n",
        "\n",
        "  #added by me\n",
        "    loss_function=torch.nn.BCELoss()\n",
        "    optimizer=torch.optim.SGD(model.parameters(),lr=0.01)\n",
        "# --------\n",
        "\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    min_loss = model.evaluate(model, data_loaders.test_loader, loss_function)\n",
        "    print(min_loss)\n",
        "    losses.append(min_loss)\n",
        "\n",
        "\n",
        "    for epoch_i in range(no_epochs):\n",
        "      #---------\n",
        "        model.train()\n",
        "        acc_loss = 0 \n",
        "        num_batch = 0\n",
        "\n",
        "        for indx,batchdict in enumerate(data_loaders.train_loader):\n",
        "              num_batch+=1\n",
        "              # Clear gradients\n",
        "              optimizer.zero_grad()\n",
        "  \n",
        "              in_batch = batchdict['input']\n",
        "                 #need to get Tensor from 'Inputs' --> model(inputs)\n",
        "              pred = torch.flatten(model(in_batch))\n",
        "    \n",
        "              label_batch = batchdict['label']\n",
        "            \n",
        "              loss = loss_function(pred, label_batch)  #need to use Tensor of 'Labels' as y_batch here\n",
        "              acc_loss += loss.item()\n",
        "\n",
        "#              losses.append(loss)\n",
        "\n",
        "              loss.backward()\n",
        "              # Update the parameters\n",
        "              optimizer.step()\n",
        "      \n",
        "        epoch_loss = acc_loss / num_batch\n",
        "        if (epoch_i+1)%10 == 0:\n",
        "             print('epoch:', epoch_i+1,',loss=',loss.item())\n",
        "             print('epoch:', epoch_i+1,',AVG loss=',epoch_loss)\n",
        "        \n",
        "        losses.append(epoch_loss)\n",
        "\n",
        "           \n",
        "    torch.save(model.state_dict(), 'saved/saved_model.pkl',_use_new_zipfile_serialization=False)\n",
        "\n",
        "\n",
        "    plt.plot(losses)\n",
        "    plt.show()\n",
        "    #Accuracy\n",
        "    #i think this is where i actually Test the Model..? where i actually USE the Test Set??  or is it supposed to be used somewhere else? \n",
        "    print(\"#\"*80)\n",
        "    print(\"ACCURACY: \")\n",
        "    data_loaders = Data_Loaders(5000) #idk if BatchSize should be the FULL amount for Test or a sub-batch?? \n",
        "    for indx,batchdict in enumerate(data_loaders.test_loader):\n",
        "      in_batch = batchdict['input']\n",
        "      y_test = batchdict['label']\n",
        "\n",
        "      with torch.no_grad():\n",
        "        y_pred=torch.flatten(model(in_batch)) #using torch.flatten again.. \n",
        "        y_pred_class=y_pred.round()\n",
        "        accuracy=(y_pred_class.eq(y_test).sum())/float(y_test.shape[0]) #what is 'y_test'?? \n",
        "        print(accuracy.item())\n",
        "\n",
        "\n",
        " \n",
        "if __name__ == '__main__':\n",
        "    no_epochs = 100 #just trying 10 for now to Test, later use 100\n",
        "    train_model(no_epochs)\n",
        "\n",
        "   \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7447869777679443\n",
            "epoch: 10 ,loss= 0.5101819634437561\n",
            "epoch: 10 ,AVG loss= 0.10703294759225007\n",
            "epoch: 20 ,loss= 0.0019457200542092323\n",
            "epoch: 20 ,AVG loss= 0.09611062504245638\n",
            "epoch: 30 ,loss= 0.0007607724983245134\n",
            "epoch: 30 ,AVG loss= 0.08777675760607466\n",
            "epoch: 40 ,loss= 0.22447136044502258\n",
            "epoch: 40 ,AVG loss= 0.08069309178581534\n",
            "epoch: 50 ,loss= 0.01118510402739048\n",
            "epoch: 50 ,AVG loss= 0.07578554610216667\n",
            "epoch: 60 ,loss= 0.11718614399433136\n",
            "epoch: 60 ,AVG loss= 0.07224064246467027\n",
            "epoch: 70 ,loss= 0.0013478214386850595\n",
            "epoch: 70 ,AVG loss= 0.06970962916350346\n",
            "epoch: 80 ,loss= 0.027983281761407852\n",
            "epoch: 80 ,AVG loss= 0.0689102752469146\n",
            "epoch: 90 ,loss= 0.0033206199295818806\n",
            "epoch: 90 ,AVG loss= 0.06735942168592043\n",
            "epoch: 100 ,loss= 0.13895948231220245\n",
            "epoch: 100 ,AVG loss= 0.06682941146185922\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################################################################################\n",
            "ACCURACY: \n",
            "0.975454568862915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d48OOATWrEO-"
      },
      "source": [
        "0.9522727131843567\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWaSD6lr1vV2",
        "outputId": "930e4179-2c24-4320-ed3c-acf6f72a9f9d"
      },
      "source": [
        "%cd ../\n",
        "#%cd assignment_part4\n",
        "%cd assignment2_part1\n",
        "\n",
        "#%cd drive/MyDrive/School/CSE 571/Assignment2/assignment_part4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/School/CSE 571/Assignment2\n",
            "/content/drive/My Drive/School/CSE 571/Assignment2/assignment2_part1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqP7I3kUhd5U"
      },
      "source": [
        "from Data_Loaders import Data_Loaders\n",
        "from Networks import Action_Conditioned_FF\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def train_model(no_epochs):\n",
        "\n",
        "    batch_size = 5  \n",
        "    data_loaders = Data_Loaders(batch_size)\n",
        "    model = Action_Conditioned_FF()\n",
        "\n",
        "    #added by me\n",
        "    loss_function=torch.nn.BCELoss()\n",
        "    optimizer=torch.optim.SGD(model.parameters(),lr=0.01)\n",
        "    # --------\n",
        "\n",
        "    losses = []\n",
        "    min_loss = model.evaluate(model, data_loaders.test_loader, loss_function)\n",
        "    losses.append(min_loss)\n",
        "    \n",
        "\n",
        "    for epoch_i in range(no_epochs):\n",
        "        model.train()\n",
        "        for idx, sample in enumerate(data_loaders.train_loader): # sample['input'] and sample['label']\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "              in_batch = batchdict['input']\n",
        "              pred = torch.flatten(model(in_batch))\n",
        "\n",
        "              label_batch = batchdict['label']\n",
        " \n",
        "              loss = loss_function(pred, label_batch)  #need to use Tensor of 'Labels' here\n",
        "\n",
        "              losses.append(loss)\n",
        "              loss.backward()\n",
        "\n",
        "              # Update the parameters\n",
        "              optimizer.step()\n",
        "\n",
        "      torch.save(model.state_dict(), 'saved/saved_model.pkl',_use_new_zipfile_serialization=False)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    no_epochs =\n",
        "    train_model(no_epochs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gQ9VU_XUyb-",
        "outputId": "1cb6a10c-7024-40b5-f198-016b215e479c"
      },
      "source": [
        "%cd ../\n",
        "%cd assignment_part4\n",
        "#%cd assignment2_part1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/School/CSE 571/Assignment2\n",
            "/content/drive/My Drive/School/CSE 571/Assignment2/assignment_part4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiAKdijtP4v8"
      },
      "source": [
        "from SteeringBehaviors import Wander, Seek\n",
        "import SimulationEnvironment as sim\n",
        "#UNCOMMENT FOR NOW from Networks import Action_Conditioned_FF\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import numpy.linalg as la\n",
        "\n",
        "\n",
        "def get_network_param(sim_env, action, scaler):\n",
        "    sensor_readings = sim_env.raycasting()\n",
        "    network_param = np.append(sensor_readings, [action, 0]) #unutilized 0 added to match shape of scaler\n",
        "    network_param = scaler.transform(network_param.reshape(1,-1))\n",
        "    network_param = network_param.flatten()[:-1]\n",
        "    network_param = torch.as_tensor(network_param, dtype=torch.float32)\n",
        "    return network_param\n",
        "\n",
        "def goal_seeking(goals_to_reach):\n",
        "    sim_env = sim.SimulationEnvironment()\n",
        "    action_repeat = 100\n",
        "    # steering_behavior = Wander(action_repeat)\n",
        "    steering_behavior = Seek(sim_env.goal_body.position)\n",
        "\n",
        "    #load model\n",
        "    model = Action_Conditioned_FF()\n",
        "    model.load_state_dict(torch.load('saved/saved_model.pkl'))\n",
        "    model.eval()\n",
        "\n",
        "    #load normalization parameters\n",
        "    scaler = pickle.load( open(\"saved/scaler.pkl\", \"rb\"))\n",
        "\n",
        "    accurate_predictions, false_positives, missed_collisions = 0, 0, 0\n",
        "    #getting 0 Missed Collisions and something below 10 for False positives gives full.\n",
        "\n",
        "    robot_turned_around = False\n",
        "    actions_checked = []\n",
        "    goals_reached = 0\n",
        "    while goals_reached < goals_to_reach:\n",
        "\n",
        "        seek_vector = sim_env.goal_body.position - sim_env.robot.body.position\n",
        "        if la.norm(seek_vector) < 50:\n",
        "            sim_env.move_goal()\n",
        "            steering_behavior.update_goal(sim_env.goal_body.position)\n",
        "            goals_reached += 1\n",
        "            continue\n",
        "\n",
        "        action_space = np.arange(-5,6)\n",
        "        actions_available = []\n",
        "        for action in action_space:\n",
        "            network_param = get_network_param(sim_env, action, scaler)\n",
        "            prediction = model(network_param)\n",
        "            if prediction.item() < .25:\n",
        "                actions_available.append(action)\n",
        "\n",
        "        if len(actions_available) == 0:\n",
        "            sim_env.turn_robot_around()\n",
        "            continue\n",
        "\n",
        "        action, _ = steering_behavior.get_action(sim_env.robot.body.position, sim_env.robot.body.angle)\n",
        "        min, closest_action = 9999, 9999\n",
        "        for a in actions_available:\n",
        "            diff = abs(action - a)\n",
        "            if diff < min:\n",
        "                min = diff\n",
        "                closest_action = a\n",
        "\n",
        "        steering_force = steering_behavior.get_steering_force(closest_action, sim_env.robot.body.angle)\n",
        "        for action_timestep in range(action_repeat):\n",
        "            _, collision, _ = sim_env.step(steering_force)\n",
        "            if collision:\n",
        "                missed_collisions+=1\n",
        "                steering_behavior.reset_action()\n",
        "                break\n",
        "    \n",
        "    print(missed_collisions)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    goals_to_reach = 10\n",
        "    goal_seeking(goals_to_reach)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nz1v5QyYx0q"
      },
      "source": [
        "# TODO: \n",
        "\n",
        "1.   Make sure what i have Works given the Entire Project instrucs (including the Robot simulation thing.. )\n",
        "- CONFIRM the amount of Data i should be training on?? (rn only 100 samples)\n",
        "--- NO! i SUBMIT only 100 samples but need to TRAIN on much much more!\n",
        "- *saved_model.pkl & scaler.pkl?* apparently 'scaler.pkl' already saved under Part 2 so just need to submit that from folder, while i need to ADD A SAVER for my Model to get SAVED_MODEL.PKL (i think, unless it's already there somewhere..)\n",
        "- apparently i don't submmit the 'train.py' but just the 'networks.py' for part 4?? if so --> i'll need to Edit Networks.py a lot to carry the train.py thing.. but idfk if the instrucs are accurate?? \n",
        "> Part 4 Evaluation: \"0-7 points. 3% of the max score will be deducted for every missed collision. 1% of the max\n",
        "score will be deducted for every false positive above 10.\n",
        "- part 3 asks you to submit Networks.py as well btw. \n",
        "\n",
        "2.   Add the \"Output_CSV\" funciton in the Part 1.. (idk if that's necessary? as i'm apparently not submitting that Part 1 at all and just the .csv...???)\n",
        "3. get the Evaluate Funciton to work & call it from Train Loop\n",
        "\n",
        "4. add a Test Function or Class OUTSIDE of the Train class/func (i think?)\n",
        "5. Edit the Part 2 BALANCING thing, but try not to use 'train_test_split' as that requires (i think) a pkg i wasn't given.. \n",
        "- maybe i can just hand-balance them?? idk if they actually need it to be balanced by code, as for part 1 i'm just submitting the CSV used... for part 2 though, i am submitting the 'DataLoaders.py'.. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "metjAHmfgZ8J"
      },
      "source": [
        "\"You will edit Train_models to train the model and make modifications to your network to minimize loss.  Dont forget you also have to output your model for submission\n",
        "\n",
        "thank you! so it is right that we initialize the loss_function and optimizer in train_model, but modify networks for the hidden layers and parameters?\n",
        "\n",
        "@Kexin: You can edit Network.py for Part 4 even after submitting it for Part 2\"\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqwHYSkRpDyp"
      },
      "source": [
        "batch_size = 16\n",
        "data_loaders = Data_Loaders(batch_size)\n",
        "\n",
        "for i,j in data_loaders.train_loader:\n",
        "    print(i,j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o55mo6RDr_3I"
      },
      "source": [
        "x = torch.randn((20,6))\n",
        "model = Action_Conditioned_FF()\n",
        "model(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UumVaUKNtEO4"
      },
      "source": [
        "class myCrazyNeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define all Layers Here\n",
        "        self.lin1 = nn.Linear(784, 30)\n",
        "        self.lin2 = nn.Linear(30, 784)\n",
        "        self.lin3 = nn.Linear(30, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Connect the layer Outputs here to define the forward pass\n",
        "        x_lin1 = self.lin1(x)\n",
        "        x_lin2 = x + self.lin2(x_lin1)\n",
        "        x_lin2 = self.lin1(x_lin2)\n",
        "        x = self.lin3(x_lin2)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3YNbWlttFjl"
      },
      "source": [
        "x = torch.randn((100,784))\n",
        "model = myCrazyNeuralNet()\n",
        "model(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN7M_2q-5dRQ"
      },
      "source": [
        "# Zipping Submissions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnWgXJ1m6RR3",
        "outputId": "2a105bc7-f376-49ff-c06b-f34befddb09f"
      },
      "source": [
        "%cd ../\n",
        "%cd Submissions\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/School/CSE 571/Assignment2\n",
            "/content/drive/My Drive/School/CSE 571/Assignment2/Submissions\n",
            "Networks.py  \u001b[0m\u001b[01;34msaved\u001b[0m/  submission.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "TQYA04Lf5iu1",
        "outputId": "3e5e9244-e9f7-49f0-9e96-80c5ea37e8cc"
      },
      "source": [
        "'''\n",
        "Zip Submission\n",
        "'''\n",
        "from zipfile import ZipFile\n",
        "\n",
        "with ZipFile('submission.zip', 'w') as myzip:\n",
        "    myzip.write('Networks.py')\n",
        "    myzip.write('saved/saved_model.pkl')\n",
        "    myzip.write('saved/scaler.pkl')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-e1e552fa0703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyzip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmyzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Networks.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmyzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved/saved_model.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmyzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saved/scaler.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, filename, arcname, compress_type, compresslevel)\u001b[0m\n\u001b[1;32m   1728\u001b[0m             )\n\u001b[1;32m   1729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m         \u001b[0mzinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZipInfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marcname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mzinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m(cls, filename, arcname)\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m         \u001b[0misdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS_ISDIR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0mmtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocaltime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved/saved_model.pkl'"
          ]
        }
      ]
    }
  ]
}